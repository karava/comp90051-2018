{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"worksheet06.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"yYr5PpNgYzUF","colab_type":"text"},"cell_type":"markdown","source":["# COMP90051 Workshop 6\n","## Convolutional neural net (CNN) in TensorFlow\n","***\n","In the previous worksheet, we implemented a logistic regression classifier for the MNIST data set in TensorFlow, which achieved a test accuracy of 92%. \n","In this worksheet, we hope to improve upon this accuracy by implementing a convolutional neural network (CNN)—a model that is more naturally suited to image data.\n","We'll assume familiarity with the TensorFlow fundamentals covered previously.\n","By the end of this worksheet you should be able to:\n","* build more complex computation graphs\n","* apply composite operators (e.g. those available under [`tf.layers`](https://www.tensorflow.org/api_docs/python/tf/layers))\n","* monitor computations in [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) (a web app that's included with TensorFlow)\n","\n","*Note: this worksheet is draws on material from the following tutorials: [link 1](https://www.tensorflow.org/tutorials/estimators/cnn) and [link 2](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/).*\n","\n","Let's begin by importing the required packages."]},{"metadata":{"id":"73dhKSijYzUG","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QnlU-FYAYzUJ","colab_type":"text"},"cell_type":"markdown","source":["### 1. Resuming from Worksheet 5\n","We're going to use the same MNIST data set as in Worksheet 5, so that we can compare the accuracy of the CNN with logistic regression.\n","\n","In Worksheet 5, we unrolled the 2D image arrays into feature vectors, as was required for logistic regression. However, here we leave the image arrays intact, as the CNN assumes images as input (it exploits spatial locality between the pixels). \n","We again apply a rescaling transformation."]},{"metadata":{"id":"V5P6KSxRYzUK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"226dd3e6-1e87-4afb-9ae2-52d4c1a96f67","executionInfo":{"status":"ok","timestamp":1536279647271,"user_tz":-600,"elapsed":2118,"user":{"displayName":"Kishan Arava","photoUrl":"//lh6.googleusercontent.com/-KXi2kDOQbNU/AAAAAAAAAAI/AAAAAAAAIn0/0tGHeQjk6Zw/s50-c-k-no/photo.jpg","userId":"117874065062765558510"}}},"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n","\n","# Rescale\n","images_train = images_train.astype('float32')/255\n","images_test = images_test.astype('float32')/255"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"1socCLykYzUO","colab_type":"text"},"cell_type":"markdown","source":["Below we define some constants related to the data set."]},{"metadata":{"id":"SwOVfh_mYzUP","colab_type":"code","colab":{}},"cell_type":"code","source":["IM_WIDTH = images_train.shape[1]      # width of an image in pixels\n","IM_HEIGHT = images_train.shape[2]     # height of an image in pixels\n","NUM_CLASSES = 10                      # number of classes (0-9)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-nznqWyqYzUR","colab_type":"text"},"cell_type":"markdown","source":["We again make use of the `DatasetIterator` defined in Worksheet 5, which provides an interface for drawing randomised mini-batches from the training set.\n","Note that we continue to use a batch size of 100 (you may consider changing this)."]},{"metadata":{"id":"LsoL_i3-YzUS","colab_type":"code","colab":{}},"cell_type":"code","source":["class DatasetIterator:\n","    \"\"\"\n","    An iterator that returns randomized batches from a data set (with features and labels)\n","    \"\"\"\n","    def __init__(self, features, labels, batch_size):\n","        assert(features.shape[0]==labels.shape[0])\n","        assert(batch_size > 0 and batch_size <= features.shape[0])\n","        self.features = features\n","        self.labels = labels\n","        self.num_instances = features.shape[0]\n","        self.batch_size = batch_size\n","        self.num_batches = self.num_instances//self.batch_size\n","        if (self.num_instances%self.batch_size!=0):\n","            self.num_batches += 1\n","        self._i = 0\n","        self._rand_ids = None\n","\n","    def __iter__(self):\n","        self._i = 0\n","        self._rand_ids = np.random.permutation(self.num_instances)\n","        return self\n","    \n","    def next(self):\n","        self.__next__(self)\n","    \n","    def __next__(self):\n","        if self.num_instances - self._i >= self.batch_size:\n","            this_rand_ids = self._rand_ids[self._i:self._i + self.batch_size]\n","            self._i += self.batch_size\n","            return self.features[this_rand_ids], self.labels[this_rand_ids]\n","        elif self.num_instances - self._i > 0:\n","            this_rand_ids = self._rand_ids[self._i::]\n","            self._i = self.num_instances\n","            return self.features[this_rand_ids], self.labels[this_rand_ids]\n","        else:\n","            raise StopIteration()\n","            \n","batch_size = 100\n","train_iterator = DatasetIterator(images_train, labels_train, batch_size)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"47xI7u4rYzUU","colab_type":"text"},"cell_type":"markdown","source":["### 2. Placeholders for data input\n","Following Worksheet 5, we again define placeholders for inputting data (images + labels) into the graph.\n","This time we group the placeholders for the images and labels under a [variable scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) called `'input'`.\n","By using variable scopes, we can simplify the graph visualisation in TensorBoard."]},{"metadata":{"id":"XTJG-G7zYzUU","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.variable_scope('input'):\n","    X = tf.placeholder(dtype=tf.float32, shape=[None, IM_WIDTH, IM_HEIGHT], name='images')\n","    Y = tf.placeholder(dtype=tf.int32, shape=[None,], name='labels')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-H15P1AqYzUX","colab_type":"text"},"cell_type":"markdown","source":["### 3. CNN architecture\n","Due to hardware and time constraints, we must limit the size of our CNN, otherwise it will take too long to train.\n","\n","For the convolutional layers, we follow the \"convolutional pyramid\" design principle—i.e. successive layers have decreasing spatial dimensions, but increasing depth. (This architecture is biologically motivated.)\n","The reduction in the spatial dimensions is achieved through max pooling.\n","\n","After the convolutional layers, we add two densely-connected layers which combine the higher-level features to make a classification.\n","We also make use of dropout (a regularization method whereby random units are removed from the network) to prevent overfitting.\n","Note that the final layer is similar to the logistic regression model (although the input differs).\n","\n","**Exercise (Advanced/Optional):** If you're interested in learning more about dropout (not examinable), you may like to read the following paper:\n","> Srivastava et al. \"Dropout: a simple way to prevent neural networks from overfitting.\" JMLR 15.1 (2014): 1929-1958. [link](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n","\n","**Architecture overview**\n","1. *Convolutional Layer #1* | 8 5×5 filters with a stride of 1 and a ReLU activation function.\n","2. *Pooling Layer #1* | Max pooling with a 2×2 filter and stride of 2 (implies pooled regions do not overlap).\n","3. *Convolutional Layer #2* | 16 5×5 filters with a stride of 1 and a ReLU activation function.\n","4. *Pooling Layer #2* | Same specs as pooling layer #1.\n","5. *Dense Layer #1* | 256 neurons, with dropout regularization rate of 0.4 (probability of 0.4 that any given element will be dropped during training)\n","6. *Dense Layer #2* | Logits Layer. 10 neurons, one for each digit target class (0–9)."]},{"metadata":{"id":"Zbk3gOqWYzUY","colab_type":"code","colab":{}},"cell_type":"code","source":["DEPTH_C1 = 8       # depth of convolutional layer #1\n","DEPTH_C2 = 16      # depth of convolutional layer #2\n","UNITS_D1 = 256     # number of neurons in dense layer #1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JuAKoRqDYzUa","colab_type":"text"},"cell_type":"markdown","source":["Fill in the missing parts of the model (Convolutional Layer #2 and Pooling Layer #2) below."]},{"metadata":{"id":"KThfp1A-YzUb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1425},"outputId":"c131fdf4-ed7c-488c-9590-04ad93fc4f8a","executionInfo":{"status":"error","timestamp":1536279693057,"user_tz":-600,"elapsed":728,"user":{"displayName":"Kishan Arava","photoUrl":"//lh6.googleusercontent.com/-KXi2kDOQbNU/AAAAAAAAAAI/AAAAAAAAIn0/0tGHeQjk6Zw/s50-c-k-no/photo.jpg","userId":"117874065062765558510"}}},"cell_type":"code","source":["with tf.variable_scope('cnn_model'):\n","    # Boolean placeholder which is set to True for training, and False for inference.\n","    # This is required to implement dropout. \n","    training_mode = tf.placeholder(dtype=tf.bool, name='training_mode')\n","    \n","    # Input Layer\n","    input_layer = tf.reshape(X, [-1, IM_WIDTH, IM_HEIGHT, 1])\n","\n","    # Convolutional Layer #1\n","    conv1 = tf.layers.conv2d(inputs=input_layer, filters=DEPTH_C1, kernel_size=[5, 5], \n","                             padding='same', activation=tf.nn.relu, use_bias=True, \n","                             name='conv_layer_1')\n","\n","    # Pooling Layer #1\n","    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, \n","                                    name='pool_layer_1')\n","\n","    # Convolutional Layer #2 \n","    conv2 = ... # fill in\n","\n","    # Pooling Layer #2\n","    pool2 = ... # fill in\n","\n","    # Dense Layer #1\n","    pool2_flat = tf.reshape(pool2, shape=[-1, 7*7*DEPTH_C2], name='pool_layer_2_flat')\n","    dense = tf.layers.dense(inputs=pool2_flat, units=UNITS_D1, activation=tf.nn.relu, \n","                            name='dense_layer_1')\n","    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=training_mode, name='dropout')\n","\n","    # Dense Layer #2 (Logits Layer)\n","    logits = tf.layers.dense(inputs=dropout, units=NUM_CLASSES, use_bias=True,\n","                             name='dense_layer_2')\n","    \n","    # Predicted labels\n","    predictions = tf.argmax(logits, axis=1)"],"execution_count":8,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     60\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 61\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got Ellipsis","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-5afea5ebb224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Dense Layer #1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mpool2_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mDEPTH_C2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pool_layer_2_flat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     dense = tf.layers.dense(inputs=pool2_flat, units=UNITS_D1, activation=tf.nn.relu, \n\u001b[1;32m     27\u001b[0m                             name='dense_layer_1')\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m               raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    508\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    215\u001b[0m                                          as_ref=False):\n\u001b[1;32m    216\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    194\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    195\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 196\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    197\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m   const_tensor = g.create_op(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    523\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[1;32m    524\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    526\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <class 'ellipsis'> to Tensor. Contents: Ellipsis. Consider casting elements to a supported type."]}]},{"metadata":{"id":"cb-pKXcAYzUf","colab_type":"text"},"cell_type":"markdown","source":["**Question:** What is the shape of the tensor output at each layer? Assume a single training instance is passed through the network. It may be helpful to review the lecture slides describing convolutional and max pooling layers.\n","*(Hint: the `padding='same'` option for `tf.layers.conv2d` adds a border of zeros around the input so that the width/height of the output = width/height of the input.)*"]},{"metadata":{"id":"VCvVaM9eYzUh","colab_type":"text"},"cell_type":"markdown","source":["### 4. Minimizing the empirical loss\n","To measure the discrepancy between the predicted class distribution and the true labels, we use the softmax cross entropy—the same loss we used for logistic regression in Worksheet 5.\n","\n","Fill in the blank below to calculate the loss from the true labels `Y` and the output of the network `logits`.\n","(Hint: The built-in losses can be found under the [`tf.losses`](https://www.tensorflow.org/api_docs/python/tf/losses) namespace.)"]},{"metadata":{"id":"vzOy4BBuYzUj","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.variable_scope('loss'):\n","    loss = ... # fill in"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-uxBcrIKYzUn","colab_type":"text"},"cell_type":"markdown","source":["To minimize the loss, we'll use the built-in Adam optimizer (it tends to converge more rapidly than gradient descent).\n","Note: we've defined the `global_step` variable to keep track of how many parameter updates have been performed."]},{"metadata":{"id":"xMwC7-V7YzUo","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.variable_scope('train'):\n","    opt = tf.train.AdamOptimizer(learning_rate=0.001)\n","    global_step = tf.Variable(0, name='global_step', trainable=False)\n","    train_op = opt.minimize(loss=loss, global_step=global_step)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3nIZo5wJYzUs","colab_type":"text"},"cell_type":"markdown","source":["### 5. Evaluation and TensorBoard summaries\n","As in Worksheet 5, we'll use accuracy to evaluate the CNN.\n","When using the built-in [`tf.metrics.accuracy`](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) implementation, `acc_op` must be called to update the accuracy—if `acc` is called then an out-of-date value (computed from internal local variables) may be returned.\n","\n","Since we're going to use TensorBoard to monitor training progress, we need to define some Summary operations (available under the [`tf.summary`](https://www.tensorflow.org/api_docs/python/tf/summary) namespace).\n","Below we define `loss_summary` and `acc_summary` to monitor the loss and accuracy.\n","Then we merge the summaries into a single Summary operation `eval_summaries` (for simplicity)."]},{"metadata":{"id":"MRdrJYKbYzUs","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.variable_scope('evaluation'):\n","    acc, acc_op = tf.metrics.accuracy(labels=Y, predictions=predictions, name='accuracy')\n","    loss_summary = tf.summary.scalar('loss', loss)\n","    acc_summary = tf.summary.scalar('accuracy', acc)\n","    eval_summaries = tf.summary.merge([loss_summary, acc_summary])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LpoXuD3EYzUv","colab_type":"text"},"cell_type":"markdown","source":["We'd also like to monitor some of the filters (a.k.a. kernels) in the first convolutional layer of the network. These will show up in the 'Images' tab in TensorBoard.\n","To do this we:\n","* extract the kernel tensor from `conv_layer_1`\n","* rescale the kernel tensor so that all values are on the unit interval\n","* transpose the kernel tensor so that the depth dimension is first\n","* define an image Summary operator"]},{"metadata":{"id":"zbwN2MspYzUw","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.variable_scope('cnn_model/conv_layer_1', reuse=True):\n","    kernel = tf.get_variable('kernel')\n","    with tf.variable_scope('visualization'):\n","        # scale weights to [0 1]\n","        x_min = tf.reduce_min(kernel)\n","        x_max = tf.reduce_max(kernel)\n","        kernel_0_to_1 = (kernel - x_min) / (x_max - x_min)\n","\n","        # to tf.summary.image format\n","        kernel_transposed = tf.transpose(kernel_0_to_1, [3, 0, 1, 2])\n","\n","        # this will display 5 filters from the 8 in conv_layer_1\n","        filter_summary = tf.summary.image('filters', kernel_transposed, max_outputs=5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"a4aeZ85SYzUz","colab_type":"text"},"cell_type":"markdown","source":["### 6. Running TensorBoard"]},{"metadata":{"id":"hC0Hc5_vYzU0","colab_type":"text"},"cell_type":"markdown","source":["Before opening a session to train the CNN, you should start TensorBoard so that you can monitor progress.\n","\n","To do this on the lab machine:\n","\n","1. Start an Anaconda Prompt in the `workshop06` directory and run the command: `python -m tensorboard.main --logdir=mnist_log --host=localhost`\n","2. Navigate to http://localhost:6006/ in your web browser.\n","3. If successful, you should see the following web page. Later on, this will be populated with useful info.\n","\n","![TensorBoard](https://screenshotscdn.firefoxusercontent.com/images/8941dbec-7dfb-4e5a-b015-225345f7615f.png)"]},{"metadata":{"id":"Fjrl-EcTYzU1","colab_type":"text"},"cell_type":"markdown","source":["### 7. Training"]},{"metadata":{"id":"oDSWPtLFYzU3","colab_type":"text"},"cell_type":"markdown","source":["We can finally start training the CNN. \n","Below we specify the log directory for TensorBoard and the number of epochs (full sweeps through the training data).\n","You'll soon see that training is slow on the CPU, so we're limited to a small number of epochs."]},{"metadata":{"id":"XiDYMWcJYzU3","colab_type":"code","colab":{}},"cell_type":"code","source":["LOG_DIR = os.path.join(os.curdir, 'mnist_log')\n","NUM_EPOCHS = 5"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XCfGAgeOYzU6","colab_type":"text"},"cell_type":"markdown","source":["We need to create an initializer for the global and local variables."]},{"metadata":{"id":"8yaAzmQzYzU7","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0cbeOMZnYzU-","colab_type":"text"},"cell_type":"markdown","source":["And open a session to run operations on the graph."]},{"metadata":{"id":"eKp_0pjPYzU-","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.Session() as sess:\n","    sess.run(init)\n","    # Instantiate writers for TensorBoard (for saving serialized summaries to disk)\n","    train_summary_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph)\n","    test_summary_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph)\n","    \n","    # Run optimizer for multiple epochs\n","    for epoch in range(NUM_EPOCHS):\n","        print(\"Starting epoch {}.\".format(epoch))\n","        for X_batch, Y_batch in train_iterator:\n","            # Run a training step\n","            _, step = sess.run([train_op, global_step],\n","                               feed_dict={X: X_batch, Y: Y_batch, training_mode: True})\n","            # Every 100 batches compute the accuracy on the training set and save the filters in the first convolutional layer\n","            if (step % 100 == 0 and step > 0):\n","                train_accuracy, eval_s, filter_s = sess.run([acc_op, eval_summaries, filter_summary], \n","                                  feed_dict={X: images_train, Y: labels_train, training_mode: False})\n","                train_summary_writer.add_summary(eval_s, global_step=step)\n","                train_summary_writer.add_summary(filter_s, global_step=step)\n","                print(\"\\tTraining accuracy at step {}: {}.\".format(step, train_accuracy))\n","            # Every 10 batches compute the accuracy on the test set.\n","            if (step % 10 == 0):\n","                test_accuracy, eval_s = sess.run([acc_op, eval_summaries], \n","                                 feed_dict={X: images_test, Y: labels_test, training_mode: False})\n","                test_summary_writer.add_summary(eval_s, global_step=step)\n","    print(\"Optimization complete.\")\n","    \n","    train_summary_writer.close()\n","    test_summary_writer.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7eT5T9noYzVC","colab_type":"text"},"cell_type":"markdown","source":["**Question:** Are 5 training epochs sufficient for this problem?"]},{"metadata":{"id":"RszO7M3YYzVC","colab_type":"text"},"cell_type":"markdown","source":["### 7. Extension activities\n","* Count the number of scalar parameters in the CNN model. How does this compare to logistic regression (from Worksheet 5)?\n","* Remove dropout from the architecture. What happens to the train/test curves? Does the model now overfit?\n","* Vary `DEPTH_C1`, `DEPTH_C2` and/or `UNITS_D1`. How do these parameters affect the goodness of fit?"]}]}